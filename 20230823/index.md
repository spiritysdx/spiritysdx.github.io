# 高效探索学习解决组合图分区问题(基于强化学习的优化算法)


## 前言

老板下指示复现两篇文章，这是其中一篇

<https://arxiv.org/pdf/2205.14105v1.pdf>

文章的原理什么的已经大部分明白了但仍然有部分懂，故而做下记录，以备后续复现或深入了解

## 原始数据

### ER40/BA40到ER500/BA500

<https://ojs.aaai.org/index.php/AAAI/article/download/5723/5579>

<https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.74.47>

分别命名为ER和BA数据集

### ER800到ER10000

<https://www.sciencedirect.com/science/article/abs/pii/S0952197612002175>

## 相关Github仓库

<https://github.com/tomdbar/ecord>

这是作者的原始仓库，非常感谢作者提供了Dockerfile和对应的数据集，使得环境依赖强制约束的非常好，开源了这么久了还能跑，依赖无需大更新

我修复了一下numpy依赖的版本(pytorch要求更高了)，然后总结了一下一步安装的脚本和步奏如下

<https://github.com/spiritysdx/ecord#edited>

进入容器后可以非常容易的一键训练出网络和使用各测试集测试，再次感谢原作者对这方面的支持

## 实验的问题

最大割（Max-Cut）问题的目标是将一个给定的无向图中的节点分成两个不相交的集合，使得割（即连接两个集合的边）的数量尽可能多。换句话说，要找到一个划分，使得划分的边的数目最大化。

形式化地说，给定一个无向图 G = (V, E)，其中 V 表示节点的集合，E 表示边的集合，Max-Cut问题的目标是找到一个节点划分 (A, B)，其中 A 和 B 是不相交的子集，使得划分的边的数量尽可能多。数学上，目标可以表示为最大化以下函数：

maximize  E(A, B) = {(u, v) ∈ E : u ∈ A 且 v ∈ B} 的大小。

## 优化思路

构建了一个新的算法 ECORD（具有递归解码的探索性组合优化），它将单个GNN预处理步骤与快速行动解码相结合，从而用简单的顶点观察和对正在进行的优化轨迹的学习表示来替代进一步的几何推理。

![https://github.com/spiritysdx/images/blob/main/20230823/1.png?raw=true](https://github.com/spiritysdx/images/blob/main/20230823/1.png?raw=true)

这里的翻转实际就是属于哪个子集，对于每个顶点，都提供以下信息：(i) 当前标签，(ii) 如果翻转该顶点会导致的切割值的立即变化（被称为“预览”）和 (iii) 自上次翻转节点以来的步数。

目标观察是当前状态和最佳观察状态之间切割值的（归一化）差异，以及当前状态中任何顶点的最大预览值。

## ECORD的伪代码

```
对于每一批次的 episodes 做如下循环：
    从分布 D 中随机采样一批 BG 图 G(V, E)。
    使用图神经网络计算每个顶点的嵌入向量。

    对于每个 episode 中的时间步 t 做如下循环：
        计算用于反向传播的时间步数 k0。
        对于批次中的每个图 Gj 做如下循环：
            根据策略选择动作 a(t)：
                - 以概率 ε 随机选择动作 a(t)。
                - 以概率 1 - ε 使用 softmax(Qθ(s(t),·) / τ) 方法选择动作 a(t)。
            
            更新解决方案集合 S(t+1)：
                - 如果 a(t) 不在 S(t) 中，则将其加入到 S(t) 中。
                - 如果 a(t) 在 S(t) 中，则将其从 S(t) 中移除。

            将元组 m(t) = (s(t-k0), ..., s(t+1), a(t-k0), ..., a(t), r(t), d) 添加到经验回放内存 M 中。
            # 这个元组包含了状态、动作、奖励等信息，用于训练模型。

            如果 t 模除 fupd 等于 0，则进行如下操作：
                从经验回放内存 M 中随机采样一批经验 M(t)。
                使用 BPTT 方法从时间步 t 到 t - k0 进行反向传播，更新模型参数 θ。

                更新目标网络的参数 θ：
                θ ← θτupd + θ(1 - τupd)
            结束
        结束
    结束
结束
```

## 优化点

ECO-DQN在每个决策步骤都使用了昂贵的GNN，再加上推理过程中的探索轨迹，导致其扩展性甚至不如S2V-DQN，并且仅用了一些手工特征来表示先前的搜索轨迹。

ECORD通过使用初始GNN嵌入，然后使用循环单元来平衡图网络提供的信息与基于学习的轨迹表示之间的关系，弥补了这一缺点。

## 相关知识点

### RNN

基于MLP多塞了一项时间t，多了一个时间轴，当前的预测值实际就是当前的观察值的预测，但当前预测值只用到了当前的隐变量ht，ht和前一个时刻的ht-1有关，ht也和前一个时间t-1中的观察值xt-1有关。(当前时间t中，输出和输入的信息不互通，但理论上预测成功的话二者是一样的)

### GRU

门控循环单元与普通的循环神经网络(RNN)之间的关键区别在于： 前者支持隐状态的门控。 这意味着模型有专门的机制来确定应该何时更新隐状态， 以及应该何时重置隐状态。

重置门有助于捕获序列中的短期依赖关系；更新门有助于捕获序列中的长期依赖关系。重置和更新门都具有可学习的参数， 因此可以对模型进行训练。

### BPTT

通过时间反向传播，要求将循环神经网络的计算图一次展开一个时间步， 以获得模型变量和参数之间的依赖关系。然后，基于链式法则，应用反向传播来计算和存储梯度。

由于通过时间反向传播是反向传播在循环神经网络中的应用方式， 所以训练循环神经网络交替使用前向传播和通过时间反向传播。 通过时间反向传播依次计算并存储上述梯度。 具体而言，存储的中间值会被重复使用，以避免重复计算。
